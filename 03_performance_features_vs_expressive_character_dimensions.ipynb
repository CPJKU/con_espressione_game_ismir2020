{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How do measurable/computable performance features relate to the expressive character dimensions?\n",
    "\n",
    "In this notebook we study whether there is a systematic relationship between the expressive character dimensions (Section 5 in the paper, notebook `02_dimensions_for_expressive_character.ipynb`) and measurable or computed performance qualities that can be extracted directly from the audio files or from the score-to-performance alignments.\n",
    "\n",
    "In the rest of this notebook, we refer to these measurable or computed performance qualities as **performance features**. We will consider three classes of performance features:\n",
    "\n",
    "1. Performance Parameters\n",
    "2. Mid-level features\n",
    "3. High-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# from scripts.performance_analysis import load_performance\n",
    "from stat_utils import (LinearModel, \n",
    "                        cod, \n",
    "                        standardize, \n",
    "                        zheng_loh_model_selection, \n",
    "                        parameter_stats, \n",
    "                        correlation,\n",
    "                        ttest_ind,\n",
    "                        pretty_print_results)\n",
    "# from scripts.utils import PIECE_NAMES, SCORE_NAMES, list_files_deep, get_file_from_id\n",
    "import partitura\n",
    "# from scipy.stats import kurtosis, skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the embeddings of the performances (i.e., their centroid) in the 4D PCA space identified in `02_dimensions_for_expressive_character.ipynb`. For convenience, we include the embeddings of each piece in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings\n",
    "df_embeddings = pd.read_csv('./perf_embeddings/perf_embeddings_final.csv')\n",
    "\n",
    "# Name of the pieces\n",
    "piece_names = np.unique(df_embeddings['piece_name'])\n",
    "\n",
    "# Names of the expressive parameters (to initialize the columns in df_embeddings)\n",
    "perf_cols = ['beat_period', \n",
    "             'loudness']\n",
    "\n",
    "stat_names = ['_mean', '_std', '_kurtosis', '_skewness']\n",
    "for col in perf_cols:\n",
    "    \n",
    "    if col != 'cresc_factor':\n",
    "        for st in stat_names:\n",
    "            df_embeddings[col + st] = np.zeros(len(df_embeddings))\n",
    "    else:\n",
    "        df_embeddings[col] = np.zeros(len(df_embeddings))\n",
    "\n",
    "all_music_ids = df_embeddings.music_id.values\n",
    "\n",
    "mid_idxs = np.array([int(np.where(all_music_ids == mi)[0]) for mi in all_music_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments_dir = './scores/ApproximateMIDI/match'\n",
    "scores_dir = './scores/MusicXML'\n",
    "loudness_dir = './beat_annotations/Loudness'\n",
    "\n",
    "alignments_list = sorted(list_files_deep(alignments_dir, full_paths=True))\n",
    "loudness_list = sorted(list_files_deep(loudness_dir, full_paths=True))\n",
    "scores_list = sorted(list_files_deep(scores_dir, full_paths=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_dataframes = dict()\n",
    "for i, piece_name in enumerate(piece_names):\n",
    "    spart = partitura.load_musicxml(os.path.join(scores_dir, SCORE_NAMES[piece_name] + '.musicxml'))\n",
    "    # df_perf = load_performance()\n",
    "    music_ids = df_embeddings[df_embeddings.piece_name==piece_name]['music_id'].values\n",
    "        \n",
    "    for mid in music_ids:\n",
    "        print('loading {0}'.format(mid))\n",
    "        match_fn = get_file_from_id(alignments_list, mid)\n",
    "        loudness_fn = get_file_from_id(loudness_list, mid)\n",
    "        df, beat_map, inv_beat_map = load_performance(spart_or_xml_fn=spart,\n",
    "                                                      match_fn=match_fn,\n",
    "                                                      loudness_fn=loudness_fn)\n",
    "        perf_dataframes[mid] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in perf_cols:\n",
    "    for mi, mix in zip(all_music_ids, mid_idxs):\n",
    "        if col != 'cresc_factor':\n",
    "            for stn, st in zip(stat_names, parameter_stats(perf_dataframes[mi][col].values)):\n",
    "                df_embeddings[col + stn][mix] = st\n",
    "        else:\n",
    "            df_embeddings[col][mix] = perf_dataframes[mi][col].values.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Parameters\n",
    "\n",
    "We consider two performance parameters, **tempo** and **dynamics**, to relate to the expressive dimensions described in the previous notebook.\n",
    "\n",
    "1. The *tempo curve* are extracted directly from the hand-corrected score-to-performance alignments by computing the inter-beat intervals (i.e., **beat period** in seconds per beat)\n",
    "\n",
    "2. We extracted *loudness curves* from the audio files using a perceptually weighted smoothing of the signal energy (thanks to Olivier Lartillot).\n",
    "\n",
    "For inter- and intra-piece comparisons, we calculate the average value, standard deviation, kurtosis and skewness of these curves:\n",
    "\n",
    "* *Average beat period*: how fast a performance is (a larger average beat period means a slower performance)\n",
    "* *Average loudness*: how loud a performance is (a larger loudness means a louder performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = []\n",
    "for cn in perf_cols:\n",
    "    if cn != 'cresc_factor':\n",
    "        input_cols += [cn + st for st in stat_names]\n",
    "    else:\n",
    "        input_cols.append(cn)\n",
    "\n",
    "target_cols = ['dim_{0}'.format(i) for i in range(1, 5)]\n",
    "\n",
    "xy = df_embeddings[input_cols + target_cols].dropna()\n",
    "x = standardize(xy[input_cols].values)\n",
    "# x = xy[input_cols].values\n",
    "for target in target_cols:\n",
    "    y = standardize(xy[target].values)\n",
    "    # y = xy[target].values\n",
    "    lm, bp_ix = zheng_loh_model_selection(x=x,\n",
    "                                          y=y,\n",
    "                                          input_names=input_cols)\n",
    "    statistics, ttest, y_hat = lm.test(x=x[:, bp_ix], \n",
    "                                y=y,\n",
    "                                return_preds=True, \n",
    "                                significance_level=0.05)\n",
    "    \n",
    "    ttest\n",
    "    corrs = [correlation(y, sl * x[:, bi]) for bi, sl in zip(bp_ix, lm.params)]\n",
    "    print(target, cod(y, y_hat, p=len(bp_ix)))\n",
    "    for sl, st, cr in zip(lm.params, statistics, corrs):\n",
    "        print(st.parameter_name, sl, st.pvalue, \n",
    "              st.reject_h0, cr,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level Features\n",
    "\n",
    "These features are perceptual qualities of music such as *articulation*, *rhytmic clarity* and *modality* that describe overall properties of musical exceprts and are intuitively clear to listeners \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_level_cols = ['melody', 'articulation', 'rhythm_complexity', \n",
    "                  'rhythm_stability', 'dissonance', 'atonality', 'mode']\n",
    "\n",
    "input_cols = mid_level_cols\n",
    "target_cols = ['dim_{0}'.format(i) for i in range(1, 5)]\n",
    "\n",
    "xy = df_embeddings[input_cols + target_cols].dropna()\n",
    "x = standardize(xy[input_cols].values)\n",
    "# x = xy[input_cols].values\n",
    "for target in target_cols:\n",
    "    y = standardize(xy[target].values)\n",
    "    # y = xy[target].values\n",
    "    lm, bp_ix = zheng_loh_model_selection(x=x,\n",
    "                                          y=y,\n",
    "                                          input_names=input_cols)\n",
    "    statistics, ttest, y_hat = lm.test(x=x[:, bp_ix], \n",
    "                                y=y,\n",
    "                                return_preds=True, \n",
    "                                significance_level=0.05)\n",
    "    corrs = [correlation(y, sl * x[:, bi]) for bi, sl in zip(bp_ix, lm.params)]\n",
    "    print(target, cod(y, y_hat, p=len(bp_ix)))\n",
    "    for sl, st, cr in zip(lm.params, statistics, corrs):\n",
    "        print(st.parameter_name, sl, st.pvalue, \n",
    "              st.reject_h0, cr,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arousal valence (Non domain adapted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings\n",
    "# df_embeddings = pd.read_csv('./perf_embeddings/perf_embeddings_w_performance.csv')\n",
    "\n",
    "# Name of the pieces\n",
    "# piece_names = np.unique(df_embeddings['piece_name'])\n",
    "\n",
    "# # Names of the expressive parameters (to initialize the columns in df_embeddings)\n",
    "# perf_cols = ['beat_period', 'articulation_log', 'velocity_trend', \n",
    "#              'velocity_dev', 'majorness', 'loudness', 'cresc_factor']\n",
    "\n",
    "# # for col in perf_cols:\n",
    "# #     df_embeddings[col] = np.zeros(len(df_embeddings))\n",
    "\n",
    "# all_music_ids = df_embeddings.music_id.values\n",
    "\n",
    "# mid_idxs = np.array([int(np.where(all_music_ids == mi)[0]) for mi in all_music_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract statistical features from dynamic arousal valence\n",
    "all_avs = list_files_deep('./audio_extracted_data/avs_3f5df_mls_1479/')\n",
    "row_header = ['music_id', 'a_max', 'v_max',\n",
    "              'a_min', 'v_min',\n",
    "              'a_std', 'v_std',\n",
    "              'a_avg', 'v_avg',\n",
    "              'a_med', 'v_med',\n",
    "             'a_kurtosis', 'v_kurtosis',\n",
    "             'a_skewness', 'v_skewness']\n",
    "vals = []\n",
    "\n",
    "for i in all_avs:\n",
    "    music_id = int(i[:2])\n",
    "    a, v = np.load(os.path.join('./audio_extracted_data/avs_3f5df_mls_1479', i))\n",
    "    a_max, v_max = max(a), max(v)\n",
    "    a_min, v_min = min(a), min(v)\n",
    "    a_std, v_std = np.std(a), np.std(v)\n",
    "    a_avg, v_avg = np.mean(a), np.mean(v)\n",
    "    a_med, v_med = np.median(a), np.median(v)\n",
    "    a_kurtosis, v_kurtosis = kurtosis(a), kurtosis(v)\n",
    "    a_skewness, v_skewness = skew(a), skew(v)\n",
    "    vals.append([music_id, a_max, v_max,\n",
    "                 a_min, v_min,\n",
    "                 a_std, v_std,\n",
    "                 a_avg, v_avg,\n",
    "                 a_med, v_med,\n",
    "                 a_kurtosis, v_kurtosis,\n",
    "                 a_skewness, v_skewness])\n",
    "    \n",
    "vals = np.vstack(vals)\n",
    "df_av = pd.DataFrame(data=vals, columns=row_header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_av.music_id = df_av.music_id.astype(int)\n",
    "df_av.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings = df_embeddings.set_index('music_id')\n",
    "df_av = df_av.set_index('music_id')\n",
    "df = pd.concat([df_embeddings, df_av], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_embeddings, df_av], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [# 'a_max', 'v_max',\n",
    "              # 'a_min', 'v_min',\n",
    "              'a_std', 'v_std',\n",
    "              'a_avg', 'v_avg',\n",
    "              # 'a_med', 'v_med',\n",
    "        'a_kurtosis', 'v_kurtosis',\n",
    "    'a_skewness', 'v_skewness'\n",
    "             ]\n",
    "\n",
    "target_cols = ['dim_{0}'.format(i) for i in range(1, 5)]\n",
    "\n",
    "xy = df[input_cols + target_cols].dropna()\n",
    "x = standardize(xy[input_cols].values)\n",
    "for target in target_cols:\n",
    "    y = standardize(xy[target].values)\n",
    "    lm, bp_ix = zheng_loh_model_selection(x=x,\n",
    "                                          y=y,\n",
    "                                          input_names=input_cols)\n",
    "    # lm = LinearModel(input_names=input_cols)\n",
    "    statistics, ttest, y_hat = lm.test(x=x[:, bp_ix], \n",
    "                                y=y,\n",
    "                                return_preds=True, \n",
    "                                significance_level=0.05)\n",
    "    corrs = [correlation(y, sl * x[:, bi]) for bi, sl in zip(bp_ix, lm.params)]\n",
    "    print(target, cod(y, y_hat, p=len(bp_ix)))\n",
    "    for sl, st, cr in zip(lm.params, statistics, corrs):\n",
    "        print(st.parameter_name, sl, st.pvalue, \n",
    "              st.reject_h0, cr,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arousal valence (domain adapted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract statistical features from dynamic arousal valence\n",
    "all_avs = list_files_deep('./audio_extracted_data/avs_5490a_mls_c5612//')\n",
    "row_header = ['music_id', 'a_max', 'v_max',\n",
    "                            'a_min', 'v_min',\n",
    "                            'a_std', 'v_std',\n",
    "                            'a_avg', 'v_avg',\n",
    "                            'a_med', 'v_med',\n",
    "             'a_kurtosis', 'v_kurtosis',\n",
    "             'a_skewness', 'v_skewness'\n",
    "             ]\n",
    "vals = []\n",
    "\n",
    "for i in all_avs:\n",
    "    music_id = int(i[:2])\n",
    "    a, v = np.load(os.path.join('./audio_extracted_data/avs_5490a_mls_c5612', i))\n",
    "    a_max, v_max = max(a), max(v)\n",
    "    a_min, v_min = min(a), min(v)\n",
    "    a_std, v_std = np.std(a), np.std(v)\n",
    "    a_avg, v_avg = np.mean(a), np.mean(v)\n",
    "    a_med, v_med = np.median(a), np.median(v)\n",
    "    a_kurtosis, v_kurtosis = kurtosis(a), kurtosis(v)\n",
    "    a_skewness, v_skewness = skew(a), skew(v)\n",
    "    vals.append([music_id, a_max, v_max,\n",
    "                    a_min, v_min,\n",
    "                    a_std, v_std,\n",
    "                    a_avg, v_avg,\n",
    "                    a_med, v_med,\n",
    "                a_kurtosis, v_kurtosis,\n",
    "                a_skewness, v_skewness])\n",
    "    \n",
    "vals = np.vstack(vals)\n",
    "df_av = pd.DataFrame(data=vals, columns=row_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_av.music_id = df_av.music_id.astype(int)\n",
    "df_av.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_av = df_av.set_index('music_id')\n",
    "df = pd.concat([df_embeddings, df_av], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_embeddings, df_av], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [# 'a_max', 'v_max',\n",
    "              # 'a_min', 'v_min',\n",
    "              'a_std', 'v_std',\n",
    "              'a_avg', 'v_avg',\n",
    "              # 'a_med', 'v_med',\n",
    "    'a_kurtosis', 'v_kurtosis',\n",
    "    'a_skewness', 'v_skewness'\n",
    "             ]\n",
    "\n",
    "target_cols = ['dim_{0}'.format(i) for i in range(1, 5)]\n",
    "# target_cols = ['dim_3']\n",
    "\n",
    "xy = df[input_cols + target_cols].dropna()\n",
    "x = standardize(xy[input_cols].values)\n",
    "for target in target_cols:\n",
    "    y = standardize(xy[target].values)\n",
    "    lm, bp_ix = zheng_loh_model_selection(x=x,\n",
    "                                          y=y,\n",
    "                                          input_names=input_cols)\n",
    "    statistics, ttest, y_hat = lm.test(x=x[:, bp_ix], \n",
    "                                y=y,\n",
    "                                return_preds=True, \n",
    "                                significance_level=0.05)\n",
    "    corrs = [correlation(y, sl * x[:, bi]) for bi, sl in zip(bp_ix, lm.params)]\n",
    "    print(target, cod(y, y_hat, p=len(bp_ix)))\n",
    "    for sl, st, cr in zip(lm.params, statistics, corrs):\n",
    "        print(st.parameter_name, sl, st.pvalue, \n",
    "              st.reject_h0, cr,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "kf = KFold(n_splits=len(y), random_state=1984, shuffle=True)\n",
    "\n",
    "tests = []\n",
    "for train_ix, test_ix in kf.split(x):\n",
    "    \n",
    "    lm = LinearModel()\n",
    "    lm.fit_predict(x[train_ix][:, bp_ix], y[train_ix])\n",
    "    \n",
    "    y_hat = lm.predict(x[test_ix][:, bp_ix])\n",
    "    tests.append(np.column_stack((y_hat, y[test_ix])))\n",
    "    \n",
    "tests = np.vstack(tests)\n",
    "\n",
    "print(cod(tests[:, 1], tests[:, 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
