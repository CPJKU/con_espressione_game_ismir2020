This repository contains code for reproducing the experiments in

>[Cancino-Chacón C.](http://www.carloscancinochacon.com), 
>[Peter, S.](https://www.jku.at/en/institute-of-computational-perception/about-us/people/silvan-david-peter/),
>[Chowdhury, S.](https://www.jku.at/en/institute-of-computational-perception/about-us/people/shreyan-chowdhury/), 
> [Aljanaki A.](https://www.jku.at/en/institute-of-computational-perception/about-us/people/anna-aljanaki/) and [Widmer G.](https://www.jku.at/en/institute-of-computational-perception/about-us/people/gerhard-widmer/)<br>
"[On the Characterization of Expressive Performance in Classical Music: First Results of the *Con Espressione* Game](https://arxiv.org/abs/2008.02194)".<br>
*In Proceedings of the International Society for Music Information Retrieval Conference*, 2020

## Abstract

A piece of music can be expressively performed, or interpreted, in a variety of ways. With the help of an online questionnaire, the [*Con Espressione* Game](http://con-espressione.cp.jku.at), we collected some 1,500 descriptions of expressive character relating to 45 performances of 9 excerpts from classical piano pieces, played by different famous pianists. More specifically, listeners were asked to describe, using freely chosen words (preferably: adjectives), how they perceive the expressive character of the different performances. In this paper, we offer a first account of this new data resource for expressive performance research, and provide an exploratory analysis, addressing three main questions:

1. how similarly do different listeners describe a performance of a piece?
2. what are the main dimensions (or axes) for expressive character emerging from this?; and
3. how do measurable parameters of a performance (e.g., tempo, dynamics) and mid- and high-level features that can be predicted by machine learning models (e.g., articulation, arousal) relate to these expressive dimensions?

The dataset that we publish along with this paper was enriched by adding hand-corrected score-to-performance alignments, as well as descriptive audio features such as tempo and dynamics curves.

## Acknowledgments

This research has received support from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme under grant agreement No. 670035 (project ["Con Espressione"](https://www.jku.at/en/institute-of-computational-perception/research/projects/con-espressione/)) and by the Research Council of Norway through its Centers of Excellence scheme, project number 262762 and the [MIRAGE project](https://www.uio.no/ritmo/english/projects/mirage/index.html), grant number 287152.

We gratefully acknowledge the effort invested by our music expert, Hans Georg Nicklaus (Anton Bruckner Private University of Music, Linz) for helping with the selection of the different performances in the dataset. We thank Olivier Lartillot for sharing the Matlab code to compute the loudness features.

![logo](https://www.jku.at/fileadmin/_processed_/4/3/csm_erc_eu_8b7e33136b.png)


